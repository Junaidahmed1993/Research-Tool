{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87099ba2",
   "metadata": {},
   "source": [
    "# SLR Automation\n",
    "\n",
    "The systamatic lireature review (SLR) is one of widely used methods to combine & sythesize novel insights from divese litreature.\n",
    "\n",
    "However, there are various tasks of SLR which researcher have to manually. These tasks include following\n",
    "\n",
    "1. Combining Data Scopus and Web of Sciennce Data\n",
    "2. Removing Duplicates\n",
    "3. Removing Articles which should not be reviewed due to lack of scope on area being studies. \n",
    "\n",
    "In this, code book, we automate the above tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852966f",
   "metadata": {},
   "source": [
    "# 1. Let's load the data.\n",
    "\n",
    "The scopus data normally is the csv format but web of science data is in excel format.\n",
    "\n",
    "It is suggested that, we should also convert web of science data into csv forma. \n",
    "We can easily do by using save as function and choosing format Comma Delimited csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a90fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the web of science data after convertibng it into the csv. \n",
    "# we have used encoding= latin-1 due to change from excel to csv.\n",
    "import pandas as pd\n",
    "web_of_science_df = pd.read_csv('savedrecs.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75012c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Publication Type', 'Authors', 'Book Authors', 'Book Editors',\n",
       "       'Book Group Authors', 'Author Full Names', 'Book Author Full Names',\n",
       "       'Group Authors', 'Article Title', 'Source Title', 'Book Series Title',\n",
       "       'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title',\n",
       "       'Conference Date', 'Conference Location', 'Conference Sponsor',\n",
       "       'Conference Host', 'Author Keywords', 'Keywords Plus', 'Abstract',\n",
       "       'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses',\n",
       "       'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred',\n",
       "       'Funding Text', 'Cited References', 'Cited Reference Count',\n",
       "       'Times Cited, WoS Core', 'Times Cited, All Databases',\n",
       "       '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher',\n",
       "       'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN',\n",
       "       'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date',\n",
       "       'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement',\n",
       "       'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page',\n",
       "       'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date',\n",
       "       'Number of Pages', 'WoS Categories', 'Web of Science Index',\n",
       "       'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations',\n",
       "       'Highly Cited Status', 'Hot Paper Status', 'Date of Export',\n",
       "       'UT (Unique WOS ID)', 'Web of Science Record'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is always nice to see the columns we have in the csv data.\n",
    "web_of_science_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d56eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can should also load the scopus data as well.\n",
    "scopus_df = pd.read_csv('scopus.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd02420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ï»¿\"Authors\"', 'Author full names', 'Author(s) ID', 'Title', 'Year',\n",
       "       'Source title', 'Volume', 'Issue', 'Art. No.', 'Page start', 'Page end',\n",
       "       'Page count', 'Cited by', 'DOI', 'Link', 'Affiliations',\n",
       "       'Authors with affiliations', 'Abstract', 'Author Keywords',\n",
       "       'Index Keywords', 'Molecular Sequence Numbers', 'Chemicals/CAS',\n",
       "       'Tradenames', 'Manufacturers', 'Funding Details', 'Funding Texts',\n",
       "       'References', 'Correspondence Address', 'Editors', 'Publisher',\n",
       "       'Sponsors', 'Conference name', 'Conference date', 'Conference location',\n",
       "       'Conference code', 'ISSN', 'ISBN', 'CODEN', 'PubMed ID',\n",
       "       'Language of Original Document', 'Abbreviated Source Title',\n",
       "       'Document Type', 'Publication Stage', 'Open Access', 'Source', 'EID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let also check its columns.\n",
    "scopus_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3963e432",
   "metadata": {},
   "source": [
    "# Data Targeted and Handle\n",
    "\n",
    "Now\n",
    "1. we will combine them. But, We need to keep in mind that, names of columns are different.\n",
    "2. we need to combine them in a way that, we retain the articles by using thier title. \n",
    "3. After combining the articles, we will remove the duplicates.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e56a3a9",
   "metadata": {},
   "source": [
    "# 1 Let's Combine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "949b366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, let combine and remove duplicates\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the data agaig. No need to load but its ok load\n",
    "scopus_df = pd.read_csv('scopus.csv', encoding='latin-1')\n",
    "web_of_science_df = pd.read_csv('savedrecs.csv', encoding='latin-1')\n",
    "\n",
    "# 2. It is highly necessary to Convert titles titles to lowercase for removing duplicates\n",
    "scopus_df['Title'] = scopus_df['Title'].str.lower()\n",
    "web_of_science_df['Article Title'] = web_of_science_df['Article Title'].str.lower()\n",
    "\n",
    "# 3. Create new dataframes which only conatins 'Title' and 'Abstract' columns.\n",
    "     #We can retain the ret of data but not necessary for now. \n",
    "scopus_title_abstract_df = scopus_df[['Title', 'Abstract']]\n",
    "web_of_science_title_abstract_df = web_of_science_df[['Article Title', 'Abstract']]\n",
    "\n",
    "# 3. Rename columns for consistency\n",
    "    # As we know that, in web of science name of column of Title is Article Title, so we need to rename it.  \n",
    "web_of_science_title_abstract_df = web_of_science_title_abstract_df.rename(columns={'Article Title': 'Title'})\n",
    "\n",
    "# 4. Now, Let Concatenate or merge both dataframes\n",
    "combined_title_abstract_df = pd.concat([scopus_title_abstract_df, web_of_science_title_abstract_df], ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "combined_title_abstract_df.to_csv('combined_title_abstract_data.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b01975",
   "metadata": {},
   "source": [
    "# 2. Let's Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6116e58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates before removal: 68\n",
      "Number of duplicates after removal: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the combined data \n",
    "combined_title_abstract_df = pd.read_csv('combined_title_abstract_data.csv', encoding='utf-8')\n",
    "\n",
    "# 2. Count the number of duplicates before removal\n",
    "total_duplicates_before = combined_title_abstract_df.duplicated(subset='Title').sum()\n",
    "\n",
    "# 3. Convert titles to lowercase and remove duplicates based on the 'Title' column\n",
    "    #Although it is handeled but it is good to have this again. \n",
    "deduplicated_combined_df = combined_title_abstract_df.drop_duplicates(subset='Title', keep=False)\n",
    "\n",
    "# 4. Count the number of duplicates after removal\n",
    "total_duplicates_after = deduplicated_combined_df.duplicated(subset='Title').sum()\n",
    "\n",
    "# 5. Save the deduplicated data to a new CSV file\n",
    "deduplicated_combined_df.to_csv('deduplicated_combined_title_abstract_data.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Print the number of duplicates before and after removal\n",
    "print(f\"Number of duplicates before removal: {total_duplicates_before}\")\n",
    "print(f\"Number of duplicates after removal: {total_duplicates_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5ea21",
   "metadata": {},
   "source": [
    "# Removing article which does not add any Value:\n",
    "    \n",
    "So, when we are searching into database. we will get the raw data.\n",
    "\n",
    "However, this data of research articles does contains articles which does not add value in answering the research questions being targered. \n",
    "\n",
    "So, Researcher have to remove them.\n",
    "\n",
    "One of widely used practicse is to read the abstract of each article and identify each article's relavance in terms of research question being developed. \n",
    "\n",
    "So, one of the way is to see whether in the abstract is discussing the key terms. \n",
    "\n",
    "For Example: Our research artilce is reviewing litreature systamatically on Project Management and Ambidextrious innovation.\n",
    "\n",
    "It means in abstract, we should be able to find key term of Ambidexrious Innovation discussed along with key terminology of Project managmenet. \n",
    "\n",
    "So, here we will have the develop function which will go into abstract and locate whether key terms of ambidextrious innovation id discussedwith the project managment or not. \n",
    "\n",
    "So, paper which discussed will be labelled as yes and paper which does not discuss will be labeled as No.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b22f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data. \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "deduplicated_combined_df = pd.read_csv('deduplicated_combined_title_abstract_data.csv', encoding='utf-8')\n",
    "\n",
    "# Keywords set for both\n",
    "# Alwats keep keywords in small case as abstract contains these into small case.\n",
    "# Be sure to set keywords. May be work on it little bit.   \n",
    "first_set_keywords = [\"ambidextrous Innovation\", \"innovation\", \"ambidextrous Innovation\", \"ambidexterity\", \"exploration\", \"exploitation\", \"exploitative innovation\", \"explorative innovation\"]\n",
    "second_set_keywords = [\"project\",\"projects\" \"project management\", \"project office\"]\n",
    "\n",
    "# Function to check if abstract contains at least one word from both sets of keywords\n",
    "def label_abstract(row):\n",
    "    abstract = row['Abstract']\n",
    "    \n",
    "    if pd.notna(abstract): \n",
    "        first_set_found = any(keyword in abstract for keyword in first_set_keywords)\n",
    "        second_set_found = any(keyword in abstract for keyword in second_set_keywords)\n",
    "        return 'Yes' if first_set_found and second_set_found else 'No'\n",
    "    else:\n",
    "        return 'No'  # Assuming that if abstract is NaN, then label should be 'No'\n",
    "\n",
    "# Apply the function to create a new 'Label' column\n",
    "deduplicated_combined_df['Label'] = deduplicated_combined_df.apply(label_abstract, axis=1)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "deduplicated_combined_df.to_csv('labeled_data.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60e23912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'Yes' labels: 99\n",
      "Number of 'No' labels: 148\n"
     ]
    }
   ],
   "source": [
    "# Let's Count the Yes and No\n",
    "\n",
    "import pandas as pd\n",
    "#1. Load the data\n",
    "labeled_df = pd.read_csv('labeled_data.csv', encoding='utf-8')\n",
    "\n",
    "# 2. Count the number of 'Yes' and 'No' labels\n",
    "count_yes = (labeled_df['Label'] == 'Yes').sum()\n",
    "count_no = (labeled_df['Label'] == 'No').sum()\n",
    "\n",
    "# 3. Print the counts\n",
    "print(f\"Number of 'Yes' labels: {count_yes}\")\n",
    "print(f\"Number of 'No' labels: {count_no}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9536c7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Assuming 'labeled_data.csv' is the name of the CSV file\n",
    "labeled_df = pd.read_csv('labeled_data.csv', encoding='utf-8')\n",
    "\n",
    "# 2. Filter rows with 'Yes' labels\n",
    "yes_df = labeled_df[labeled_df['Label'] == 'Yes']\n",
    "\n",
    "# 3. Save the 'Yes' labeled data to a new CSV file\n",
    "yes_df.to_csv('yes_labeled_data.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
